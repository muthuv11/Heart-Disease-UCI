{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "To See we you can find any other trends in heart data to predict certain cardiovascular events or find any clear indications of heart health.\n",
    "\n",
    "* Identify the attributes which explains the prediction\n",
    "\n",
    "### Dataset reference:\n",
    "https://www.kaggle.com/ronitf/heart-disease-uci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Required Packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import pylab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import sklearn.preprocessing as preproc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "class Data:\n",
    "    def __init__(self, path, file,categorical_cols, numerical_cols, boolean_cols,target_col,for_prediction):\n",
    "        self.path = path\n",
    "        self.file = file\n",
    "\n",
    "        self.categorical_cols = list(categorical_cols)\n",
    "        self.numerical_cols = list(numerical_cols)\n",
    "        self.boolean_cols = list(boolean_cols)\n",
    "        self.target_col = list(target_col)\n",
    "        self.col_list = categorical_cols+numerical_cols+boolean_cols\n",
    "        self.for_prediction = for_prediction\n",
    "        self.for_prediction = for_prediction\n",
    "        \n",
    "        self.DS = self._read_csv_to_DF()\n",
    "        #self._exportEDA(self.DS)\n",
    "        self._chk_duplicate_clean()\n",
    "               \n",
    "\n",
    "    def _read_csv_to_DF(self):  #read CSV to dataframe\n",
    "        DS = pd.read_csv(self.path+'/'+self.file)\n",
    "        print(str(DS.shape[0]) + ' observations with '+str(DS.shape[1])+' Features loaded \\n')\n",
    "        return DS\n",
    "    \n",
    "    def _exportEDA(self): #export feature analysis profiling \n",
    "        profile = self.DS.profile_report(title='Pandas Profiling Report - EDA_output.html \\n')\n",
    "        profile.to_file(output_file=\"EDA_output.html\")\n",
    "        pass\n",
    "    \n",
    "    def _chk_duplicate_clean(self): #remove duplicate records\n",
    "        dup = self.DS[self.DS.duplicated()].copy()\n",
    "        print('removed duplicate observations- '+ str(dup.shape[0])+'\\n')\n",
    "        self.DS.drop_duplicates(keep='first',inplace=True) \n",
    "        pass\n",
    "    \n",
    "    def _splitdata(self): #split train/test dataset (for_prediction= true, data for prediction been passed)\n",
    "        X= self.DS.drop('target',axis=1)\n",
    "        y= self.DS['target']       \n",
    "        if self.for_prediction:\n",
    "            test_set = X\n",
    "            test_target = y\n",
    "        else:\n",
    "            train_set,test_set,train_target,test_target = train_test_split(X,y,test_size=.3,random_state=42)\n",
    "        return train_set,test_set,train_target,test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-181-9cb59f0a3f59>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-181-9cb59f0a3f59>\"\u001b[1;36m, line \u001b[1;32m40\u001b[0m\n\u001b[1;33m    def __applyFeatureTransformation()\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Transformation:\n",
    "    def __init__(self,dataset,col_logT,col_boxCoxT,categorical_cols, for_prediction):\n",
    "        self.dataset = dataset\n",
    "        self.for_prediction = for_prediction\n",
    "        self.col_logT = list(col_logT)\n",
    "        self.col_boxCoxT = list(col_boxCoxT)\n",
    "        self.categorical_cols = list(categorical_cols)      \n",
    "        self.ColLambda ={}\n",
    "        self.__applyFeatureTransformation()\n",
    "           \n",
    "        \n",
    "    def _storeTransLamda(self, colname,lamda):\n",
    "        self.ColLambda.update({colname:lamda})\n",
    "        pass\n",
    "        \n",
    "    def _apply_BoxCoxTrans(self, colname): # BoxCox Transformation, returns lamda for test set\n",
    "        newcol = colname+'BCox'\n",
    "        if self.for_prediction:\n",
    "            fitted_lambda = dic.get(colname)\n",
    "            trn_data = stats.boxcox(self.dataset[colname], fitted_lambda)\n",
    "        else:\n",
    "            trn_data,fitted_lambda = stats.boxcox(self.dataset[colname])\n",
    "            self.dataset[newcol] = trn_data\n",
    "            self.dataset.drop(columns = colname,inplace =True)\n",
    "            print(colname +' feature transformed: '+newcol)\n",
    "            self._storeTransLamda(colname,fitted_lambda)\n",
    "        \n",
    "    def _applyStandardization(self,colname):\n",
    "        newcol=colname+'-Tstd'\n",
    "        self.dataset[newcol] = preproc.StandardScaler().fit_transform(self.dataset[[colname]])\n",
    "        self.dataset.drop(columns = colname,inplace =True)\n",
    "        print(colname +' feature transformed: '+newcol)\n",
    "    \n",
    "    def _dummyEncode_category(self, colname): #Dummy Encode Categorical features\n",
    "        newcol = colname+'-'\n",
    "        DS_dummytype = pd.get_dummies(self.dataset[colname],prefix=newcol)\n",
    "        self.dataset= pd.concat((self.dataset,DS_dummytype),axis=1)\n",
    "        self.dataset = self.dataset.drop(columns=colname,axis=1)\n",
    "        \n",
    "    def __applyFeatureTransformation()\n",
    "        if self.col_boxCoxT:\n",
    "            print('BoxCox Transformation')\n",
    "            for colname in col_boxCoxT:\n",
    "                self._apply_BoxCoxTrans(colname)\n",
    "            print('Lamda values \\n'+str(self.ColLambda)+'\\n')\n",
    "\n",
    "        if self.col_logT:\n",
    "            print('Log Transformation')\n",
    "            for colname in col_logT:\n",
    "                self._applyStandardization(colname)\n",
    "\n",
    "        if self._dummyEncode_category:\n",
    "            print('Dummy Encoded'+ str(categorical_cols) + ' \\n')\n",
    "            for colname in categorical_cols:\n",
    "                self._dummyEncode_category(colname)\n",
    "        \n",
    "\n",
    "#apply_standardization(DS,'thalachBCox',keepexistingcol = False)  \n",
    "     \n",
    "    def _returnTransformation(self):\n",
    "        return self.dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 observations with 14 Features loaded \n",
      "\n",
      "removed duplicate observations- 1\n",
      "\n",
      "BoxCox Transformation\n",
      "thalach feature transformed: thalachBCox\n",
      "age feature transformed: ageBCox\n",
      "trestbps feature transformed: trestbpsBCox\n",
      "Lamda values \n",
      "{'thalach': 2.0203142406883083, 'age': 1.5424134424242417, 'trestbps': -0.8611426256437702}\n",
      "\n",
      "Log Transformation\n",
      "chol feature transformed: chol-Tstd\n",
      "Dummy Encoded['ca', 'cp', 'exang', 'slope', 'thal', 'restecg'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>fbs</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>thalachBCox</th>\n",
       "      <th>ageBCox</th>\n",
       "      <th>trestbpsBCox</th>\n",
       "      <th>chol-Tstd</th>\n",
       "      <th>ca-_0</th>\n",
       "      <th>ca-_1</th>\n",
       "      <th>ca-_2</th>\n",
       "      <th>...</th>\n",
       "      <th>slope-_0</th>\n",
       "      <th>slope-_1</th>\n",
       "      <th>slope-_2</th>\n",
       "      <th>thal-_0</th>\n",
       "      <th>thal-_1</th>\n",
       "      <th>thal-_2</th>\n",
       "      <th>thal-_3</th>\n",
       "      <th>restecg-_0</th>\n",
       "      <th>restecg-_1</th>\n",
       "      <th>restecg-_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17621.380811</td>\n",
       "      <td>183.801655</td>\n",
       "      <td>1.138032</td>\n",
       "      <td>-0.861420</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22496.021628</td>\n",
       "      <td>116.145933</td>\n",
       "      <td>1.143688</td>\n",
       "      <td>-0.769640</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>13694.418672</td>\n",
       "      <td>269.942680</td>\n",
       "      <td>1.142435</td>\n",
       "      <td>-0.494303</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>14046.908745</td>\n",
       "      <td>304.047573</td>\n",
       "      <td>1.144774</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16448.428815</td>\n",
       "      <td>176.557760</td>\n",
       "      <td>1.144568</td>\n",
       "      <td>-1.301960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sex  fbs  oldpeak   thalachBCox     ageBCox  trestbpsBCox  chol-Tstd  \\\n",
       "124    0    0      0.0  17621.380811  183.801655      1.138032  -0.861420   \n",
       "72     1    0      0.0  22496.021628  116.145933      1.143688  -0.769640   \n",
       "15     0    0      1.6  13694.418672  269.942680      1.142435  -0.494303   \n",
       "10     1    0      1.2  14046.908745  304.047573      1.144774  -0.127186   \n",
       "163    1    0      0.0  16448.428815  176.557760      1.144568  -1.301960   \n",
       "\n",
       "     ca-_0  ca-_1  ca-_2     ...      slope-_0  slope-_1  slope-_2  thal-_0  \\\n",
       "124      1      0      0     ...             0         0         1        0   \n",
       "72       1      0      0     ...             0         0         1        0   \n",
       "15       1      0      0     ...             0         1         0        0   \n",
       "10       1      0      0     ...             0         0         1        0   \n",
       "163      0      0      0     ...             0         0         1        0   \n",
       "\n",
       "     thal-_1  thal-_2  thal-_3  restecg-_0  restecg-_1  restecg-_2  \n",
       "124        0        1        0           0           1           0  \n",
       "72         0        1        0           1           0           0  \n",
       "15         0        1        0           0           1           0  \n",
       "10         0        1        0           0           1           0  \n",
       "163        0        1        0           0           1           0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data load \n",
    "path = \"D:/Datasets/HeartDiseaseUCI/heart-disease-uci\"\n",
    "file ='heart.csv'\n",
    "\n",
    "numeric_cols = ['age','chol','oldpeak','trestbps','thalach']\n",
    "categorical_cols = ['ca','cp','exang','slope','thal','restecg']\n",
    "boolean_cols = ['exang','fbs','sex']\n",
    "target_col = ['target']\n",
    "\n",
    "data = Data(path,file,categorical_cols, numeric_cols, boolean_cols,target_col,for_prediction=0)\n",
    "train_set,test_set,train_target,test_target = data._splitdata()\n",
    "\n",
    "# Normalize the train features - apply tansformations\n",
    "col_logT= ['chol']\n",
    "col_boxCoxT= ['thalach','age','trestbps']\n",
    "\n",
    "trainsetobj = Transformation(train_set,col_logT,col_boxCoxT,categorical_cols,for_prediction = False)\n",
    "transformedset =  trainsetobj._returnTransformation()\n",
    "transformedset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance - Random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= DS.drop('target',axis=1)\n",
    "y=DS['target']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "train_model  = RandomForestClassifier( max_depth=5, min_samples_split=2\n",
    "                                      , min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_features='auto', max_leaf_nodes=2, min_impurity_decrease=0.0,\n",
    "                                      min_impurity_split=None, bootstrap=True, oob_score=True\n",
    "                                      , n_jobs=None, random_state=42, verbose=0, warm_start=False, class_weight=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=2,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=True, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7203791469194313\n"
     ]
    }
   ],
   "source": [
    "print(train_model.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(train_model.feature_importances_, index= X.columns).nlargest(50).plot(kind = 'barh', figsize=(10,10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_standardization(DS,colname,colReplace = True):\n",
    "    newcolname=colname+'-Tstd'\n",
    "    DS[newcolname] = preproc.StandardScaler().fit_transform(DS[[colname]])\n",
    "    if(colReplace ==True):\n",
    "        DS.drop(columns = colname,inplace =True)\n",
    "        print('feature dropped: '+colname)\n",
    "        \n",
    "\n",
    "#apply_standardization(DS,'thalachBCox',keepexistingcol = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= ['cholBCox','thalachBCox','ageBCox','trestbpsBCox','oldpeak']\n",
    "for i in cols:\n",
    "    apply_standardization(DS,i,colReplace= True)\n",
    "DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_LogTransformation(DS,colname, keepexistingcol= False):\n",
    "    newcol = colname+'Tlog'\n",
    "    DS[newcol] = np.log(DS[colname]+1)\n",
    "    if(keepexistingcol ==False):\n",
    "        DS.drop(columns = colname,inplace =True)\n",
    "        print('feature dropped: '+colname)\n",
    "\n",
    "    \n",
    "#apply_LogTransformation(DS,'chol',keepexistingcol= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= DS.drop('target',axis=1)\n",
    "y=DS['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preproc\n",
    "#Min-Max Scaling\n",
    "DS['Min-Maxage'] = preproc.minmax_scale(DS[['age']])\n",
    "# Standardization\n",
    "DS['Standardization'] = preproc.StandardScaler().fit_transform(DS[['age']])\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Plot Original Price\n",
    "DS['age'].hist(ax=ax1, bins=50)\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_xlabel(\"Original age\", fontsize=10)\n",
    "ax1.set_ylabel(\"Frequency\", fontsize=14)\n",
    "\n",
    "# Plot Min-Max Scaling on Price\n",
    "DS['Min-Max'].hist(ax=ax2, bins=50, color='r')\n",
    "ax2.tick_params(labelsize=14)\n",
    "ax2.set_xlabel(\"Min-Max age\", fontsize=10)\n",
    "\n",
    "# Plot Standardized Scaling on Price\n",
    "DS['Standardization'].hist(ax=ax3, bins=50, color='g')\n",
    "ax3.tick_params(labelsize=14)\n",
    "ax3.set_xlabel(\"Standarized age\", fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.drop(columns=['Standardization','age'],inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sex',hue='slope',data=DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='age', y='chol', data=DS, kind='scatter', color = 'b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('age', 'chol', data=DS, hue='target', fit_reg=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"age\", y=\"sex\", hue=\"target\",data=DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='chol', data= DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(DS['chol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA & Visualization summary\n",
    "* Age- population set is covering the adults (since min is 29 and max is 80) - so the target patients are of adult & older people\n",
    "* CA- number of vessels blocked in heart (ordinal - so it says from factor plot 1/3 of the population set is having 0 vessel block is having hear disease.) (major population are having 0 vessels – so need the check how value 0 correlates with positive of heart disease)\n",
    "* Chol – cholesterol is interval data- also discovered that cholesterol level is normal/high/low based on age. So planning to do impute additional feature with age&cholesterol = (high/normal/low)\n",
    "* cp: The chest pain experienced – ordinal\n",
    "* exang: Exercise induced angina (yes or No) – it means pain experienced during exercise, only minor fraction of patients seems to have pain and diagnosed heart disease. (Negative correlation – can check value later in heatmap)\n",
    "* fbs: The person's fasting blood sugar (> 120 mg/dl) 1 = true; 0 = false) (more than 230 patients among 300 are having sugar level less than 120  so…….(May be imbalanced data?))\n",
    "* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria) , most of patients have normal ecg value 0 at rest but no heart disease (need to check the importance of this feature) \n",
    "* trestbps: The person's resting blood pressure (checked and there are 2 values systolic and diastolic – not sure the given BP ready is which one. Need to check more on that – can add extra feature to say low/normal/high based on the value)\n",
    "* Thalch: maximum heart rate achieved – Distribution Normal\n",
    "* Sex: more male patients seems to have heart disease diagnosed at early 40s.(not sure if this is a correct plot- need to check the syntax ) Female patient doesn’t seem to have any relation wrt to age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a correlation identified - lets proceed\n",
    "Choletrol seems to be left skwed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "train_model  = RandomForestClassifier( max_depth=5, min_samples_split=2\n",
    "                                      , min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_features='auto', max_leaf_nodes=2, min_impurity_decrease=0.0,\n",
    "                                      min_impurity_split=None, bootstrap=True, oob_score=True\n",
    "                                      , n_jobs=None, random_state=42, verbose=0, warm_start=False, class_weight=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_model.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(train_model.feature_importances_, index= X.columns).nlargest(50).plot(kind = 'barh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predit = train_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Train accuracy : '+ str (accuracy_score(y_train, train_model.predict(X_train))))\n",
    "print('Test accuracy : '+ str (accuracy_score(y_test, Y_predit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, Y_predit)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(cm))\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "Y_predicted_Prob = train_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, Y_predicted_Prob)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for diabetes classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(random_state=0)\n",
    "lr_model.fit(X_train,y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy score : '+ str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy score : '+ str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state = 1)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy score : '+ str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(X)\n",
    "feature_scaled = scaler1.transform(X)\n",
    "\n",
    "#now apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca1 =  PCA(n_components =4)\n",
    "pca1.fit(feature_scaled)\n",
    "feature_scaled_pca =  pca1.transform(feature_scaled)\n",
    "print(\"shape of scaled and pca features\",np.shape(feature_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (type(cancer.target))\n",
    "target_list = y.tolist()\n",
    "print (type(target_list))\n",
    "feature_scaled_pca_X0 = feature_scaled_pca[:, 0]\n",
    "feature_scaled_pca_X1 = feature_scaled_pca[:, 1]\n",
    "feature_scaled_pca_X2 = feature_scaled_pca[:, 2]\n",
    "feature_scaled_pca_X3 = feature_scaled_pca[:, 3]\n",
    "\n",
    "labels = target_list\n",
    "colordict = {0:'brown', 1:'darkslategray'}\n",
    "piclabel = {0:'Positive', 1:'Negative'}\n",
    "markers = {0:'o', 1:'*'}\n",
    "alphas = {0:0.3, 1:0.4}\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "plt.subplot(1,2,1)\n",
    "for l in np.unique(labels):\n",
    "    ix = np.where(labels==l)\n",
    "    plt.scatter(feature_scaled_pca_X0[ix], feature_scaled_pca_X1[ix], c=colordict[l], \n",
    "               label=piclabel[l], s=40, marker=markers[l], alpha=alphas[l])\n",
    "plt.xlabel(\"First Principal Component\", fontsize=15)\n",
    "plt.ylabel(\"Second Principal Component\", fontsize=15)\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for l1 in np.unique(labels):\n",
    "    ix1 = np.where(labels==l1)\n",
    "    plt.scatter(feature_scaled_pca_X2[ix1], feature_scaled_pca_X3[ix1], c=colordict[l1], \n",
    "               label=piclabel[l1], s=40, marker=markers[l1], alpha=alphas[l1])\n",
    "plt.xlabel(\"Third Principal Component\", fontsize=15)\n",
    "plt.ylabel(\"Fourth Principal Component\", fontsize=15)\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "plt.savefig('heartDisease_PCAs.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
